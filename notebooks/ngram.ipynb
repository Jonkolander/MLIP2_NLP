{"metadata":{"kernelspec":{"name":"python395jvsc74a57bd02d972ce83b011c0fbc8bf1befcdd55f31c9249b362a9ff124184ebc36a0909c1","display_name":"Python 3.9.5 64-bit"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.9.5","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"metadata":{"interpreter":{"hash":"2d972ce83b011c0fbc8bf1befcdd55f31c9249b362a9ff124184ebc36a0909c1"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import spacy\n","import json\n","import csv\n","import pickle\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","import matplotlib.pyplot as plt\n","import itertools\n","from tqdm.notebook import tqdm\n","from math import floor\n","from random import shuffle\n","from sklearn.model_selection import train_test_split\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    print(f\"{dirname} contains {len(filenames)} files\")\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-22T10:11:25.341460Z","iopub.execute_input":"2021-05-22T10:11:25.341814Z","iopub.status.idle":"2021-05-22T10:11:37.844937Z","shell.execute_reply.started":"2021-05-22T10:11:25.341741Z","shell.execute_reply":"2021-05-22T10:11:37.843816Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["# GLOBAL SETTINGS"],"metadata":{}},{"cell_type":"code","source":["saved_model_filepath = None # SET TO 'None' if you want to train from scratch!\n","save_model = True\n","test_size = 0.2"],"metadata":{"execution":{"iopub.status.busy":"2021-05-22T10:11:47.370411Z","iopub.execute_input":"2021-05-22T10:11:47.370947Z","iopub.status.idle":"2021-05-22T10:11:47.375611Z","shell.execute_reply.started":"2021-05-22T10:11:47.370912Z","shell.execute_reply":"2021-05-22T10:11:47.374599Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["# utilities"],"metadata":{}},{"cell_type":"code","source":["import string \n","import re\n","\n","def clean_text(text):\n","    '''\n","    Converts all text to lower case, Removes special characters, emojis and multiple spaces\n","    text - Sentence that needs to be cleaned\n","    '''\n","    text = ''.join([k for k in text if k not in string.punctuation])\n","    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()).strip()\n","    text = re.sub(' +', ' ', text)\n","    emoji_pattern = re.compile(\"[\"\n","                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                               \"]+\", flags=re.UNICODE)\n","    text = emoji_pattern.sub(r'', text)\n","    return text"],"metadata":{"execution":{"iopub.status.busy":"2021-05-22T10:12:00.356313Z","iopub.execute_input":"2021-05-22T10:12:00.356773Z","iopub.status.idle":"2021-05-22T10:12:00.365039Z","shell.execute_reply.started":"2021-05-22T10:12:00.356733Z","shell.execute_reply":"2021-05-22T10:12:00.363851Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# load data\n"],"metadata":{}},{"cell_type":"code","source":["def load_train_data():\n","    training_data = []\n","    \n","    # open the csv with id's, data labels, etc. and append the json files to it\n","    files = []\n","    train_dir = '../input/coleridgeinitiative-show-us-the-data/train' # location of the training json files\n","    df = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/train.csv') # location of the training csv file (does not contain the actual texts)\n","    for i in df.index:\n","        file_id = df['Id'][i]\n","        filename = f\"{file_id}.json\"\n","        filepath = os.path.join(train_dir, filename)\n","        with open(filepath) as json_file:\n","            file = json.loads(json_file.read())\n","            files.append(file)\n","    df['file'] = files\n","    \n","    return df\n","\n","df = load_train_data()\n","df.describe()"],"metadata":{"execution":{"iopub.status.busy":"2021-05-22T10:12:27.592910Z","iopub.execute_input":"2021-05-22T10:12:27.593277Z","iopub.status.idle":"2021-05-22T10:14:19.012316Z","shell.execute_reply.started":"2021-05-22T10:12:27.593247Z","shell.execute_reply":"2021-05-22T10:14:19.011173Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                          Id  \\\n","count                                  19661   \n","unique                                 14316   \n","top     170113f9-399c-489e-ab53-2faf5c64c5bc   \n","freq                                      22   \n","\n","                                      pub_title  \\\n","count                                     19661   \n","unique                                    14271   \n","top     Science and Engineering Indicators 2014   \n","freq                                         22   \n","\n","                                            dataset_title dataset_label  \\\n","count                                               19661         19661   \n","unique                                                 45           130   \n","top     Alzheimer's Disease Neuroimaging Initiative (A...          ADNI   \n","freq                                                 6144          3673   \n","\n","       cleaned_label                                               file  \n","count          19661                                              19661  \n","unique           130                                              14301  \n","top             adni  [{'section_title': '', 'text': 'National Scien...  \n","freq            3673                                                 22  "],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>pub_title</th>\n      <th>dataset_title</th>\n      <th>dataset_label</th>\n      <th>cleaned_label</th>\n      <th>file</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>19661</td>\n      <td>19661</td>\n      <td>19661</td>\n      <td>19661</td>\n      <td>19661</td>\n      <td>19661</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>14316</td>\n      <td>14271</td>\n      <td>45</td>\n      <td>130</td>\n      <td>130</td>\n      <td>14301</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>170113f9-399c-489e-ab53-2faf5c64c5bc</td>\n      <td>Science and Engineering Indicators 2014</td>\n      <td>Alzheimer's Disease Neuroimaging Initiative (A...</td>\n      <td>ADNI</td>\n      <td>adni</td>\n      <td>[{'section_title': '', 'text': 'National Scien...</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>22</td>\n      <td>22</td>\n      <td>6144</td>\n      <td>3673</td>\n      <td>3673</td>\n      <td>22</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["print(df['cleaned_label'][20])"],"metadata":{"execution":{"iopub.status.busy":"2021-05-22T10:19:23.432942Z","iopub.execute_input":"2021-05-22T10:19:23.433327Z","iopub.status.idle":"2021-05-22T10:19:23.438648Z","shell.execute_reply.started":"2021-05-22T10:19:23.433296Z","shell.execute_reply":"2021-05-22T10:19:23.437819Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["national education longitudinal study\n"]}]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["0    [{'section_title': 'What is this study about?'...\n1    [{'section_title': 'November 2004', 'text': 'D...\n2    [{'section_title': 'Differences in Outcomes fo...\n3    [{'section_title': 'Abstract', 'text': 'Federa...\n4    [{'section_title': 'Abstract', 'text': 'This a...\nName: file, dtype: object\n[{'dataset_label': 'National Education Longitudinal Study', 'dataset_title': 'National Education Longitudinal Study', 'cleaned_label': 'national education longitudinal study'}, {'dataset_label': 'National Education Longitudinal Study', 'dataset_title': 'National Education Longitudinal Study', 'cleaned_label': 'national education longitudinal study'}, {'dataset_label': 'National Education Longitudinal Study', 'dataset_title': 'National Education Longitudinal Study', 'cleaned_label': 'national education longitudinal study'}, {'dataset_label': 'National Education Longitudinal Study', 'dataset_title': 'National Education Longitudinal Study', 'cleaned_label': 'national education longitudinal study'}, {'dataset_label': 'National Education Longitudinal Study', 'dataset_title': 'National Education Longitudinal Study', 'cleaned_label': 'national education longitudinal study'}]\n"]}],"source":["X_train = df[\"file\"]\n","y_train = df[\"dataset_label\"]\n","y_train = [{\n","    \"dataset_label\": df[\"dataset_label\"][index],\n","    \"dataset_title\": df[\"dataset_title\"][index],\n","    \"cleaned_label\": df[\"cleaned_label\"][index]\n","} \n","    for index in y_train.index]\n","\n","print(X_train[:5])\n","print(y_train[:5])"]},{"cell_type":"code","source":["def format_dataframe_for_spacy(xs, ys):\n","    '''\n","    xs - array of samples, where each sample is an array of dictionaries, where each dictionary has a `text` and `section_title` key-value pair\n","    ys - array of strings, where the i'th index is the dataset label corresponding to the i'th sample in `xs`\n","    '''\n","    data = []\n","    pb = tqdm(total=len(xs))\n","    for x, y in zip(xs, ys):\n","        for section in x:\n","            # each section contains a 'section_title' and a 'text' key, for now we only use 'text'\n","            text = section['text']\n","            \n","            # tokenize the text into sentences\n","            sentences = sent_tokenize(text)\n","\n","            # !IMPORTANT TODO: Adding padding to the dataset title removes about 1/3rd of the training data. probably not good\n","            for sentence in sentences:\n","                # Only use a sentence as a training sample IF it contains a dataset label\n","                if y in sentence:\n","                    sample = sentence.replace(y, '<DATASET>')\n","                    data.append(sample)\n","        pb.update(1)\n","    pb.close()\n","    return data\n","\n","spacy_training_data = format_dataframe_for_spacy(X_train, [y[\"dataset_label\"] for y in y_train])\n","print(spacy_training_data[0])"],"metadata":{"execution":{"iopub.status.busy":"2021-05-22T10:19:26.455716Z","iopub.execute_input":"2021-05-22T10:19:26.456274Z","iopub.status.idle":"2021-05-22T10:19:27.697518Z","shell.execute_reply.started":"2021-05-22T10:19:26.456238Z","shell.execute_reply":"2021-05-22T10:19:27.696463Z"},"trusted":true},"execution_count":79,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/19661 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aec800f9d04e41f3818779abeea9a726"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["This study used data from the <DATASET> (NELS:88) to examine the effects of dual enrollment programs for high school students on college degree attainment.\n"]}]},{"cell_type":"code","source":["TRAIN_DATA = spacy_training_data\n","print(TRAIN_DATA[0])"],"metadata":{"execution":{"iopub.status.busy":"2021-05-22T10:19:46.132398Z","iopub.execute_input":"2021-05-22T10:19:46.132805Z","iopub.status.idle":"2021-05-22T10:19:46.136546Z","shell.execute_reply.started":"2021-05-22T10:19:46.132769Z","shell.execute_reply":"2021-05-22T10:19:46.135472Z"},"trusted":true},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["This study used data from the <DATASET> (NELS:88) to examine the effects of dual enrollment programs for high school students on college degree attainment.\n"]}]},{"cell_type":"code","source":["import nltk\n","from nltk.util import ngrams\n","from nltk import word_tokenize\n","\n","textdata = TRAIN_DATA[0].split()\n","    \n","def process_text(text):\n","    text = text.lower()\n","    text = text.replace(',', ' ')\n","    text = text.replace('/', ' ')\n","    text = text.replace('(', ' ')\n","    text = text.replace(')', ' ')\n","    text = text.replace('.', ' ')\n","    text = text.replace(';', ' ')\n","    text = text.replace(':', ' ')\n","    text = text.replace('-', ' ')\n"," \n","    # Convert text string to a list of words\n","    return text.split()\n","\n","def generate_ngrams(words_list, n):\n","    ngrams_list = []\n"," \n","    for num in range(0, len(words_list)):\n","        ngram = ' '.join(words_list[num:num + n])\n","        ngrams_list.append(ngram)\n"," \n","    return ngrams_list\n","\n","\n","def getAllNgrams(data, n):\n","    ngrams_Set = []\n","    for index in range(len(data)):\n","        wordList = process_text(data[index])\n","        ngrams = generate_ngrams(wordList, n)\n","        ngrams_Set.append(ngrams)\n","    \n","    return ngrams_Set\n","\n","bigrams_data = getAllNgrams(TRAIN_DATA, 4)\n","# print(bigrams_data[0])\n","\n","total_filtered = []\n","for index in range(len(bigrams_data)):\n","    total_filtered.append([x for x in bigrams_data[index] if '<dataset>' in x])\n","\n","fdist = nltk.FreqDist(list(itertools.chain(*total_filtered))).most_common(20)\n","for k,v in fdist:\n","    print(k.split(' '),v)"],"metadata":{"execution":{"iopub.status.busy":"2021-05-22T11:20:29.293307Z","iopub.execute_input":"2021-05-22T11:20:29.293705Z","iopub.status.idle":"2021-05-22T11:20:33.778672Z","shell.execute_reply.started":"2021-05-22T11:20:29.293670Z","shell.execute_reply":"2021-05-22T11:20:33.777560Z"},"trusted":true,"tags":[]},"execution_count":89,"outputs":[{"output_type":"stream","name":"stdout","text":["['disease', 'neuroimaging', 'initiative', '<dataset>'] 3529\n['<dataset>'] 3057\n['from', 'the', '<dataset>', 'database'] 1811\n['data', 'from', 'the', '<dataset>'] 1538\n['obtained', 'from', 'the', '<dataset>'] 1428\n['<dataset>', 'database', 'adni', 'loni'] 1092\n['neuroimaging', 'initiative', '<dataset>', 'database'] 1080\n['the', '<dataset>'] 971\n['<dataset>', 'of', '1988', 'nels'] 900\n['<dataset>', 'kindergarten', 'class', 'of'] 764\n['for', 'education', 'statistics', '<dataset>'] 717\n['national', '<dataset>', 'of', '1988'] 677\n['the', '<dataset>', 'database', 'adni'] 666\n['<dataset>', 'database'] 642\n['<dataset>', 'was', 'launched', 'in'] 640\n['primary', 'goal', 'of', '<dataset>'] 577\n['<dataset>', 'has', 'been', 'to'] 571\n['of', '<dataset>', 'has', 'been'] 551\n['goal', 'of', '<dataset>', 'has'] 538\n['the', '<dataset>', 'was', 'launched'] 524\n"]}]}]}