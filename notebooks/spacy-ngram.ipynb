{"metadata":{"kernelspec":{"name":"python395jvsc74a57bd02d972ce83b011c0fbc8bf1befcdd55f31c9249b362a9ff124184ebc36a0909c1","display_name":"Python 3.9.5 64-bit"},"language_info":{"name":"python","version":"3.9.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"metadata":{"interpreter":{"hash":"2d972ce83b011c0fbc8bf1befcdd55f31c9249b362a9ff124184ebc36a0909c1"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# TODO\n","0. REMOVE SPACE AT THE END OF SOME FOUND LABELS FOR NAIVE MODEL\n","0. IMPORTANT: make submission and evaluation logic the same function because now they are seperate pieces of logic that do the same thing\n","0. add levenshtein distance to naive model to get best predictions\n","0. during the submit process, save all new dataset names in the naive model.\n","0. after NLP prediction, use levenshtein distance to find previously known dataset. If distance is small enough, use the known dataset\n","1. do data pre-processing (cleaning etc) IMPORTANT: cleaning the whole text may result in inconsistencies between the dataset name in our cleaned text, and the dataset label in the answers csv\n","3. add naive baseline (with a buffer of all known datasets) and combine with spacy model\n","4. add SciBERT as a third model\n"],"metadata":{}},{"cell_type":"markdown","source":["# settings"],"metadata":{}},{"cell_type":"code","source":["# notebook settings\n","is_submission = False\n","use_gpu = True\n","\n","# nlp model settings\n","use_nlp = False # whether to use the nlp model at all\n","train_nlp = True # whether to train the nlp\n","save_model = True # whether to save the trained model\n","saved_model_filepath = None # whether to load a previously trained model\n","\n","# other settings\n","test_size = 0.2 \n","\n","if use_nlp and (not train_nlp and not saved_model_filepath):\n","    raise Exception(\"ERROR: nlp model must be loaded from file OR trained\")"],"metadata":{"execution":{"iopub.status.busy":"2021-06-07T16:50:06.349736Z","iopub.execute_input":"2021-06-07T16:50:06.350331Z","iopub.status.idle":"2021-06-07T16:50:06.361856Z","shell.execute_reply.started":"2021-06-07T16:50:06.350216Z","shell.execute_reply":"2021-06-07T16:50:06.361070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# imports"],"metadata":{}},{"cell_type":"code","source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import json\n","import spacy\n","import csv\n","import re\n","import pickle\n","import string\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","import os\n","import networkx as nx\n","import matplotlib.pyplot as plt\n","import itertools\n","from tqdm.notebook import tqdm\n","from math import floor\n","from random import shuffle\n","from time import sleep\n","from sklearn.model_selection import train_test_split\n","\n","if use_gpu:\n","    using_gpu = spacy.require_gpu()\n","    print(f\"using gpu: {using_gpu}\")\n","else:\n","    print(\"not using gpu\\n\")\n","\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    print(f\"{dirname} contains {len(filenames)} files\")"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-06-07T16:50:06.363543Z","iopub.execute_input":"2021-06-07T16:50:06.363927Z","iopub.status.idle":"2021-06-07T16:50:23.606355Z","shell.execute_reply.started":"2021-06-07T16:50:06.363892Z","shell.execute_reply":"2021-06-07T16:50:23.605486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# load data\n"],"metadata":{}},{"cell_type":"code","source":["def load_train_data():\n","    training_data = []\n","    \n","    # open the csv with id's, data labels, etc. and append the json files to it\n","    files = []\n","    train_dir = '../input/coleridgeinitiative-show-us-the-data/train' # location of the training json files\n","    df = pd.read_csv('../input/coleridgeinitiative-show-us-the-data/train.csv') # location of the training csv file (does not contain the actual texts)\n","    for i in df.index:\n","        file_id = df['Id'][i]\n","        filename = f\"{file_id}.json\"\n","        filepath = os.path.join(train_dir, filename)\n","        with open(filepath) as json_file:\n","            file = json.loads(json_file.read())\n","            files.append(file)\n","    df['file'] = files\n","    \n","    return df\n","\n","df = load_train_data()\n","df.describe()\n","df.info()"],"metadata":{"execution":{"iopub.status.busy":"2021-06-07T16:50:23.608258Z","iopub.execute_input":"2021-06-07T16:50:23.608621Z","iopub.status.idle":"2021-06-07T16:51:39.805936Z","shell.execute_reply.started":"2021-06-07T16:50:23.608584Z","shell.execute_reply":"2021-06-07T16:51:39.805011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create train and validation set"],"metadata":{}},{"cell_type":"code","source":["def fill_graph(ids, labels, use_pseudonyms):\n","    graph = nx.Graph()\n","    \n","    amount_of_publications = 0\n","    amount_of_datasets = 0\n","    id_to_name = {}\n","    \n","    # fill the graph with all ids and labels (duplicates will not be added twice)\n","    for i in range(len(ids)):\n","        converted_id = ids[i]\n","        converted_label = labels[i]\n","        \n","        if use_pseudonyms:\n","            if not ids[i] in id_to_name:\n","                id_to_name[ids[i]] = 'P{}'.format(amount_of_publications)\n","                amount_of_publications += 1\n","\n","            if not labels[i] in id_to_name:\n","                id_to_name[labels[i]] = 'D{}'.format(amount_of_datasets)\n","                amount_of_datasets += 1\n","\n","            converted_id = id_to_name[ids[i]]\n","            converted_label = id_to_name[labels[i]]\n","            \n","        if not graph.has_node(converted_id):\n","            graph.add_node(converted_id)\n","            \n","        if not graph.has_node(converted_label):\n","            graph.add_node(converted_label)\n","            \n","        graph.add_edge(converted_id, converted_label)\n","    \n","    undirected_graph = graph.to_undirected()\n","    return undirected_graph, id_to_name"],"metadata":{"execution":{"iopub.status.busy":"2021-06-07T16:51:39.807525Z","iopub.execute_input":"2021-06-07T16:51:39.808030Z","iopub.status.idle":"2021-06-07T16:51:39.816309Z","shell.execute_reply.started":"2021-06-07T16:51:39.807989Z","shell.execute_reply":"2021-06-07T16:51:39.815287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def visualize_graph(graph):\n","    node_color = []\n","    for node in graph.nodes(data=True):\n","        node_type = node[0][0]\n","        if node_type == 'D':\n","            node_color.append('#8cfffb')\n","        elif node_type == 'P':\n","            node_color.append('#c4ff0e')\n","\n","    plt.figure(1,figsize=(40, 40)) \n","    nx.draw(graph, node_size=2500, with_labels=True, font_weight='bold', node_color=node_color)\n","    plt.savefig(\"subgraph.pdf\")"],"metadata":{"execution":{"iopub.status.busy":"2021-06-07T16:51:39.817787Z","iopub.execute_input":"2021-06-07T16:51:39.818176Z","iopub.status.idle":"2021-06-07T16:51:39.827716Z","shell.execute_reply.started":"2021-06-07T16:51:39.818136Z","shell.execute_reply":"2021-06-07T16:51:39.826930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["threshold = 20\n","\n","def combine_small_graphs(sub_graphs):\n","    deleted = []\n","    combined = nx.Graph()\n","    for i, sg in enumerate(sub_graphs):\n","        if (sg.number_of_nodes() < threshold):\n","            combined.add_edges_from(sg.edges(data=True))\n","            combined.add_nodes_from(sg.nodes(data=True))\n","            deleted.append(sg)\n","            \n","    for sg in deleted:\n","        sub_graphs.remove(sg)\n","        \n","    return sub_graphs, combined"],"metadata":{"execution":{"iopub.status.busy":"2021-06-07T16:51:39.829282Z","iopub.execute_input":"2021-06-07T16:51:39.829733Z","iopub.status.idle":"2021-06-07T16:51:39.839980Z","shell.execute_reply.started":"2021-06-07T16:51:39.829693Z","shell.execute_reply":"2021-06-07T16:51:39.839111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def remove_dataset_nodes(sub_graphs):\n","    new_subgraphs = []\n","    for i, sg in enumerate(sub_graphs):\n","        copy_sg = nx.Graph()\n","        copy_sg.add_nodes_from(sg.nodes(data=True))\n","        copy_sg.add_edges_from(sg.edges(data=True))\n","        nodes_to_remove = []\n","        for node in copy_sg.nodes():\n","            if \"D\" in node:\n","                nodes_to_remove.append(node)\n","        \n","        copy_sg.remove_nodes_from(nodes_to_remove)\n","        new_subgraphs.append(copy_sg)\n","        \n","    return new_subgraphs"],"metadata":{"execution":{"iopub.status.busy":"2021-06-07T16:51:39.841278Z","iopub.execute_input":"2021-06-07T16:51:39.841680Z","iopub.status.idle":"2021-06-07T16:51:39.850325Z","shell.execute_reply.started":"2021-06-07T16:51:39.841640Z","shell.execute_reply":"2021-06-07T16:51:39.849347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def find_nearest_split(amounts, split):\n","    target = split * sum(amounts)\n","    \n","    combinations = []\n","    for i in range(1, len(amounts)):\n","        combinations.append([list(x) for x in itertools.combinations(amounts, i)])\n","        \n","    combinations = list(itertools.chain(*combinations))\n","    summed_combinations = [sum(combination) for combination in combinations]\n","    \n","    nearest_value = min(summed_combinations, key=lambda x:abs(x - target))\n","    nearest_value_index = summed_combinations.index(nearest_value)\n","    return combinations[nearest_value_index]"],"metadata":{"execution":{"iopub.status.busy":"2021-06-07T16:51:39.853451Z","iopub.execute_input":"2021-06-07T16:51:39.853919Z","iopub.status.idle":"2021-06-07T16:51:39.860940Z","shell.execute_reply.started":"2021-06-07T16:51:39.853878Z","shell.execute_reply":"2021-06-07T16:51:39.860061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def custom_train_test_split(df, use_pseudonyms = False):\n","    ids = df[\"Id\"]\n","    labels = df[\"dataset_label\"]\n","    undirected_graph, id_to_name = fill_graph(ids, labels, use_pseudonyms)\n","\n","    # extract subgraphs \n","    sub_graphs = [undirected_graph.subgraph(c) for c in nx.connected_components(undirected_graph)]\n","    \n","    # merge smaller ones into one bigger graph\n","    sub_graphs, combined = combine_small_graphs(sub_graphs)\n","    sub_graphs.append(combined)\n","    \n","    # remove all dataset nodes from the graph\n","    sub_graphs = remove_dataset_nodes(sub_graphs)\n","    \n","    # find the nearest split\n","    nodes = [sub_graph.nodes() for sub_graph in sub_graphs]\n","    flattened_nodes = list(itertools.chain(*nodes))\n","    amount_of_nodes = [len(sub_graph.nodes()) for sub_graph in sub_graphs]\n","    split = find_nearest_split(amount_of_nodes, split=test_size)\n","    \n","    test_nodes = list(itertools.chain(*[nodes[amount_of_nodes.index(s)] for s in split]))\n","    train_nodes = [n for n in flattened_nodes if n not in test_nodes]\n","    \n","    if use_pseudonyms:\n","        # convert D and P names to ids again using the conversion dictionary\n","        test_nodes = [[k for k,v in id_to_name.items() if v == test_node][0] for test_node in test_nodes]\n","        train_nodes = [[k for k,v in id_to_name.items() if v == train_node][0] for train_node in train_nodes]\n","    \n","    train_df = df[df[\"Id\"].isin(train_nodes)]\n","    test_df = df[df[\"Id\"].isin(test_nodes)]\n","    \n","    return train_df[\"file\"], test_df[\"file\"], train_df[\"dataset_label\"], test_df[\"dataset_label\"]"],"metadata":{"execution":{"iopub.status.busy":"2021-06-07T16:51:39.862869Z","iopub.execute_input":"2021-06-07T16:51:39.863371Z","iopub.status.idle":"2021-06-07T16:51:39.874611Z","shell.execute_reply.started":"2021-06-07T16:51:39.863334Z","shell.execute_reply":"2021-06-07T16:51:39.873772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if is_submission:\n","    X_train = df[\"file\"]\n","    y_train = [{\n","        \"dataset_label\": df[\"dataset_label\"][index],\n","        \"dataset_title\": df[\"dataset_title\"][index],\n","        \"cleaned_label\": df[\"cleaned_label\"][index]\n","    } \n","        for index in df.index]\n","else:\n","    X_train, X_val, y_train, y_val = custom_train_test_split(df)\n","    y_train = [{\n","        \"dataset_label\": df[\"dataset_label\"][index],\n","        \"dataset_title\": df[\"dataset_title\"][index],\n","        \"cleaned_label\": df[\"cleaned_label\"][index]\n","    } \n","        for index in y_train.index]\n","    y_val = [{\n","        \"dataset_label\": df[\"dataset_label\"][index],\n","        \"dataset_title\": df[\"dataset_title\"][index],\n","        \"cleaned_label\": df[\"cleaned_label\"][index]\n","    } \n","        for index in y_val.index]\n","    \n","    print(X_train[:5])\n","    print(y_train[:5])"],"metadata":{"execution":{"iopub.status.busy":"2021-06-07T16:51:39.875991Z","iopub.execute_input":"2021-06-07T16:51:39.876405Z","iopub.status.idle":"2021-06-07T16:51:41.405739Z","shell.execute_reply.started":"2021-06-07T16:51:39.876347Z","shell.execute_reply":"2021-06-07T16:51:41.404607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def format_dataframe_for_spacy(xs, ys):\n","    '''\n","    xs - array of samples, where each sample is an array of dictionaries, where each dictionary has a `text` and `section_title` key-value pair\n","    ys - array of strings, where the i'th index is the dataset label corresponding to the i'th sample in `xs`\n","    '''\n","    data = []\n","    pb = tqdm(total=len(xs))\n","    for x, y in zip(xs, ys):\n","        for section in x:\n","            # each section contains a 'section_title' and a 'text' key, for now we only use 'text'\n","            text = section['text']\n","            \n","            # tokenize the text into sentences\n","            sentences = sent_tokenize(text)\n","\n","            # !IMPORTANT TODO: Adding padding to the dataset title removes about 1/3rd of the training data. probably not good\n","            for sentence in sentences:\n","                # Only use a sentence as a training sample IF it contains a dataset label\n","                if y in sentence:\n","                    start_index = sentence.find(y)\n","                    end_index = start_index + len(y)\n","                    entity = (start_index, end_index, 'DATASET')\n","                    entities = [entity]\n","                    sample = (sentence, {'entities': entities})\n","                    data.append(sample)\n","        pb.update(1)\n","    pb.close()\n","    return data\n","\n","spacy_training_data = format_dataframe_for_spacy(X_train, [y[\"dataset_label\"] for y in y_train])\n","print(spacy_training_data[0])"],"metadata":{"execution":{"iopub.status.busy":"2021-06-07T16:51:41.406995Z","iopub.execute_input":"2021-06-07T16:51:41.407344Z","iopub.status.idle":"2021-06-07T16:58:56.988887Z","shell.execute_reply.started":"2021-06-07T16:51:41.407306Z","shell.execute_reply":"2021-06-07T16:58:56.987920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_test_data():\n","    buffer = []\n","    ids = []\n","    for dirname, _, filenames in os.walk('../input/coleridgeinitiative-show-us-the-data/test'):\n","        for filename in filenames:\n","            filepath = os.path.join(dirname, filename)\n","            with open(filepath) as json_file:\n","                file = json.loads(json_file.read())\n","                file_id = filename.replace(\".json\", \"\")\n","                ids.append(file_id)\n","                buffer.append(file)\n","    return buffer, ids\n","\n","X_test, ids = load_test_data()\n","print(ids[:5])"],"metadata":{"execution":{"iopub.status.busy":"2021-06-07T16:58:56.990141Z","iopub.execute_input":"2021-06-07T16:58:56.990506Z","iopub.status.idle":"2021-06-07T16:58:57.023257Z","shell.execute_reply.started":"2021-06-07T16:58:56.990467Z","shell.execute_reply":"2021-06-07T16:58:57.022297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Spacy NER model"],"metadata":{}},{"cell_type":"code","source":["def create_blank_nlp():\n","    nlp = spacy.load(\"en_core_web_sm\")\n","    nlp.remove_pipe(\"ner\")\n","    ner = nlp.create_pipe(\"ner\")\n","    nlp.add_pipe(ner, last=True)\n","    ner.add_label('DATASET')\n","    return nlp"],"metadata":{"execution":{"iopub.status.busy":"2021-06-07T17:05:05.037055Z","iopub.execute_input":"2021-06-07T17:05:05.037366Z","iopub.status.idle":"2021-06-07T17:05:05.041966Z","shell.execute_reply.started":"2021-06-07T17:05:05.037335Z","shell.execute_reply":"2021-06-07T17:05:05.041047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datetime as dt\n","\n","def load_nlp_from_file(filename):\n","    with open(filename, 'rb') as pickle_file:\n","        model = pickle.load(pickle_file)\n","        return model\n","    \n","def save_nlp_to_file(model):\n","    filename = \"model.sav\"\n","    pickle.dump(model, open(filename, 'wb'))\n","    return filename"],"metadata":{"execution":{"iopub.status.busy":"2021-06-07T17:05:06.553140Z","iopub.execute_input":"2021-06-07T17:05:06.553473Z","iopub.status.idle":"2021-06-07T17:05:06.558090Z","shell.execute_reply.started":"2021-06-07T17:05:06.553442Z","shell.execute_reply":"2021-06-07T17:05:06.557279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"source":["from spacy.util import minibatch, compounding\n","from tqdm import trange\n","\n","# !IMPORTANT TODO: this ignores wrong alignment warnings, this reduces the training data, so look into this\n","spacy.warnings.filterwarnings(\"ignore\", message=r\"\\[W030\\]\", category=UserWarning)\n","\n","EPOCHS = 2\n","\n","def train_nlp_model(nlp, training_data):\n","    optimizer = nlp.begin_training()\n","    for epoch in range(EPOCHS):\n","        losses = {}\n","        batch_gen = minibatch(training_data, size=compounding(4.0, 32.0, 1.001))\n","        batches = list(batch_gen)\n","        pb = tqdm(total=len(batches))\n","        for batch_index, batch in enumerate(batches):\n","            pb.set_description(f'epoch: {epoch} | batch {batch_index + 1} of {len(batches)}')\n","            texts, annotations = zip(*batch)\n","            nlp.update(\n","                texts,  # batch of texts\n","                annotations,  # batch of annotations\n","                drop=0.1,  # dropout - make it harder to memorise data\n","                losses=losses,\n","            )\n","            pb.update(1)\n","        pb.close()\n","        print(f\"Losses at iteration {epoch} - {dt.datetime.now()} {losses}\")\n","    return nlp\n","\n","nlp = None\n","if use_nlp and train_nlp:\n","    print(\"Training NLP model from scratch...\")\n","    nlp = create_blank_nlp()\n","    nlp = train_nlp_model(nlp, spacy_training_data)\n","    print(\"NLP model trained\")\n","    if save_model:\n","        print(\"Saving trained NLP model...\")\n","        save_nlp_to_file(nlp)\n","        print(\"NLP model saved\")\n","elif use_nlp and saved_model_filepath:\n","    print(f\"Loading NLP model from: {saved_model_filepath}...\")\n","    nlp = load_nlp_from_file(saved_model_filepath)\n","    print(\"Loaded NLP model\")"],"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-06-07T17:05:08.657002Z","iopub.execute_input":"2021-06-07T17:05:08.657330Z","iopub.status.idle":"2021-06-07T17:08:16.688788Z","shell.execute_reply.started":"2021-06-07T17:05:08.657298Z","shell.execute_reply":"2021-06-07T17:08:16.687924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"source":["doc = nlp(\"This study used data from the National Education Longitudinal Study (NELS:88) to examine the effects of dual enrollment programs for high school students on college degree attainment.\")\n","print('Ents:', doc.ents)\n","for ent in doc.ents:\n","    print(ent.text)\n","\n","print('Size:', len(X_val))\n","\n","for index, x in enumerate(X_val):\n","    entities = []\n","    for section in x:\n","        text = section['text']\n","        sentences = sent_tokenize(text)\n","        for sentence in sentences:\n","            doc = nlp(sentence)\n","            for ent in doc.ents:\n","                entities.append(ent.text)\n","                \n","    print(index, entities)"],"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-06-07T17:08:24.076952Z","iopub.execute_input":"2021-06-07T17:08:24.077288Z","iopub.status.idle":"2021-06-07T17:09:22.204329Z","shell.execute_reply.started":"2021-06-07T17:08:24.077257Z","shell.execute_reply":"2021-06-07T17:09:22.202187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Functions used by kaggle"],"metadata":{}},{"cell_type":"code","source":["def jaccard(str1, str2): \n","    a = set(str1.lower().split()) \n","    b = set(str2.lower().split())\n","    c = a.intersection(b)\n","    return float(len(c)) / (len(a) + len(b) - len(c))\n","\n","def clean_label(txt):\n","    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())"],"metadata":{"execution":{"iopub.status.busy":"2021-06-07T17:02:54.826216Z","iopub.status.idle":"2021-06-07T17:02:54.826977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["cleaning functions from other notebook (for cleaning sections, not predictions/labels)"],"metadata":{}},{"cell_type":"code","source":["def clean_section(text):\n","    '''\n","    Converts all text to lower case, Removes special charecters, emojis and multiple spaces\n","    text - Sentence that needs to be cleaned\n","    '''\n","    text = ''.join([k for k in text if k not in string.punctuation])\n","    text = re.sub('r[^\\w\\s]', ' ', str(text).lower()).strip()\n","    lem = nltk.stem.wordnet.WordNetLemmatizer()\n","    text = lem.lemmatize(text)\n","    text = re.sub('[^A-Za-z0-9]+', ' ', str(text).lower()) # !IMPORTANT this was added by us\n","    \n","    return text"],"metadata":{"execution":{"iopub.status.busy":"2021-06-07T17:02:54.828194Z","iopub.status.idle":"2021-06-07T17:02:54.828814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Naive model\n","keeps track of all dataset labels in our training data, and checks if a sample contains one of these, and if so, returns it"],"metadata":{}},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["regexes = [\n","    # r'(.*?) database',\n","    # r'initiative (.*?)',\n","    # r'(.*?) data',\n","    # r'(.*?) dataset',\n","    # r'(.*?) cohort',\n","    # r'(.*?) kindergarten',\n","    # r'national (.*?)',\n","    # r'from the (.*?)',\n","    # r'in the (.*?)',\n","    r'the (.*?) database',\n","    # r'of the (.*?)',\n","    # r'(.*?) of 1988',\n","    r'the (.*?) dataset',\n","    r'initiative (.*?) database',\n","    r'the (.*?) cohort',\n","    # r'goal of (.*?)',\n","    # r'on the (.*?)',\n","    r'the (.*?) data',\n","    # r'the national (.*?)',\n","    r'from the (.*?) database',\n","    # r'data from the (.*?)',\n","    # r'obtained from the (.*?)',\n","    # r'for education statistics (.*?)',\n","    # r'primary goal of (.*?)',\n","    r'the (.*?) was launched',\n","]"]},{"cell_type":"code","source":["class NaiveModel():\n","    def __init__(self, X_train, y_train):\n","        self.labels = list()\n","        self.X_train = X_train\n","        self.y_train = y_train\n","        \n","    def train(self):\n","        for y in self.y_train:\n","            new_labels = [\n","                clean_section(y[\"dataset_title\"]),\n","                clean_section(y[\"dataset_label\"]),\n","                y[\"cleaned_label\"]\n","            ]\n","            for label in new_labels:\n","                if not label in self.labels:\n","                    self.labels.append(label)\n","        \n","    def predict(self, text):\n","        found_labels = []\n","        for regex in regexes:\n","            title_search = re.search(regex, text, re.IGNORECASE)\n","            if title_search:\n","                title = title_search.group(1)\n","                if title and title not in found_labels:\n","                    found_labels.append(title)\n","\n","        if len(found_labels) == 0:\n","            for label in self.labels:\n","                if label in text:\n","                    if label not in found_labels: # no duplicates\n","                        found_labels.append(label)\n","\n","        return found_labels"],"metadata":{"execution":{"iopub.status.busy":"2021-06-07T17:02:54.830031Z","iopub.status.idle":"2021-06-07T17:02:54.830658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create naive model and train it"],"metadata":{}},{"cell_type":"code","source":["naive = NaiveModel(X_train, y_train)\n","naive.train()\n","print(f'Naive model has collected: {len(naive.labels)} unique labels')\n","print(list(naive.labels)[:5])"],"metadata":{"execution":{"iopub.status.busy":"2021-06-07T17:02:54.831785Z","iopub.status.idle":"2021-06-07T17:02:54.832365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Test naive model"],"metadata":{}},{"cell_type":"code","source":["from tqdm import trange\n","\n","def f_score(tp, fp, fn, beta=0.5):\n","    precision = tp / (tp + fp)\n","    recall = tp / (tp + fn)\n","    score = (1+beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n","    return score\n","\n","def evaluate(preds_val, y_val):\n","    '''\n","    preds_val - an array containing n strings, where each string is a prediction, or multiple predictions seperated by a '|' character\n","    y_val - an array containing n labels, it is important to note these are the DATASET_LABELS, and not the CLEANED_LABEL (for predictions, these must go through Kaggle's clean_text function)\n","    '''\n","    tp = 0\n","    fp = 0\n","    fn = 0\n","    for pred, y in zip(preds_val, y_val):\n","        if pred:\n","            preds = pred.split(\"|\")\n","            for pred in preds:\n","                j = jaccard(pred, y['cleaned_label'])\n","                if j > 0.5:\n","                    tp += 1\n","                else:\n","                    fp += 1\n","        else:\n","            print(\"no pred:\", preds, \"|\", y['cleaned_label'])\n","            fn += 1\n","    return f_score(tp, fp, fn)\n","\n","def predict_one(naive, file, filter_subsets = False):\n","    preds = []\n","    for section in file:\n","        text = clean_label(section['text'])\n","        text = clean_section(text)\n","        labels = naive.predict(text)\n","        if labels:\n","            preds += labels\n","    \n","    # remove labels that are a subset of other labels to reduce False Positives    \n","    if filter_subsets:\n","        for i in preds:\n","            is_subset = False\n","            for j in preds:\n","                if i in j and i != j:\n","                    is_subset = True\n","            if is_subset:\n","                preds.remove(i)\n","    \n","    labels = [clean_label(label) for label in preds]\n","    labels = set(labels)\n","    labels = list(labels)\n","    labels = \"|\".join(labels)\n","    return labels\n","\n","def test(model, xs, ys):\n","    predictions = []\n","    pbar = tqdm(total=len(xs)) # Progress bar\n","    for x in xs:\n","        pbar.update(1) # Update progress bar\n","        new_prediction = predict_one(naive, x, filter_subsets=False)\n","        predictions.append(new_prediction)\n","        \n","    pbar.close()\n","    print(evaluate(predictions, y_val))\n","\n","if not is_submission:\n","    test(naive, X_val, y_val)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-07T17:02:54.833647Z","iopub.status.idle":"2021-06-07T17:02:54.834239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Generate output file (WORK IN PROGRESS)"],"metadata":{}},{"cell_type":"markdown","source":["predict function that takes in some models, and tries to predict a dataset title for each"],"metadata":{}},{"cell_type":"code","source":["def submit(ids, xs):\n","    buffer = {}\n","    for file_id, file in zip(ids, xs):\n","        dataset_label = predict_one(naive, file)\n","        buffer[file_id] = dataset_label\n","    \n","    ids = buffer.keys()\n","    predictions = buffer.values()\n","    data = {\"Id\": ids, \"PredictionString\": predictions }\n","    df = pd.DataFrame(data=data)\n","    df.to_csv('submission.csv', index=False)\n","    \n","if is_submission:\n","    submit(ids, X_test)"],"metadata":{"execution":{"iopub.status.busy":"2021-06-07T17:02:54.835353Z","iopub.status.idle":"2021-06-07T17:02:54.836090Z"},"trusted":true},"execution_count":null,"outputs":[]}]}